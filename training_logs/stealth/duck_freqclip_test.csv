step,total_loss,learning_rate
0,3.232421875,0.01
1,3.23046875,0.01
2,3.384765625,0.01
3,2.890625,0.01
4,3.90234375,0.01
5,3.384765625,0.01
6,3.876953125,0.01
7,3.337890625,0.01
8,3.0390625,0.01
9,3.166015625,0.01
10,4.2890625,0.01
11,3.880859375,0.01
12,2.951171875,0.01
13,3.173828125,0.01
14,3.2265625,0.01
15,3.982421875,0.01
16,3.591796875,0.01
17,3.3828125,0.01
18,3.640625,0.01
19,3.208984375,0.01
20,3.541015625,0.01
21,3.625,0.01
22,4.05859375,0.01
23,3.095703125,0.01
24,4.1796875,0.01
25,3.408203125,0.01
26,3.205078125,0.01
27,3.912109375,0.01
28,3.34765625,0.01
29,3.669921875,0.01
30,3.859375,0.01
31,3.048828125,0.01
32,4.1953125,0.01
33,3.04296875,0.01
34,2.783203125,0.01
35,4.10546875,0.01
36,3.47265625,0.01
37,3.75,0.01
38,3.30078125,0.01
39,4.17578125,0.01
40,3.4609375,0.01
41,3.58984375,0.01
42,2.98046875,0.01
43,3.0859375,0.01
44,3.05078125,0.01
45,4.14453125,0.01
46,3.83984375,0.01
47,3.287109375,0.01
48,3.71875,0.01
49,3.494140625,0.01
50,4.51171875,0.01
51,3.41015625,0.01
52,4.21875,0.01
53,4.18359375,0.01
54,3.966796875,0.01
55,3.53125,0.01
56,3.5,0.01
57,2.9296875,0.01
58,3.982421875,0.01
59,3.556640625,0.01
60,3.080078125,0.01
61,3.072265625,0.01
62,3.265625,0.01
63,3.6875,0.01
64,3.8125,0.01
65,3.685546875,0.01
66,4.33203125,0.01
67,3.486328125,0.01
68,4.26171875,0.01
69,3.017578125,0.01
70,3.25390625,0.01
71,2.98828125,0.01
72,3.978515625,0.01
73,3.798828125,0.01
74,3.341796875,0.01
75,3.490234375,0.01
76,3.044921875,0.01
77,3.322265625,0.01
78,3.419921875,0.01
79,3.52734375,0.01
80,2.841796875,0.01
81,3.25,0.01
82,3.26171875,0.01
83,3.416015625,0.01
84,3.935546875,0.01
85,2.990234375,0.01
86,3.5390625,0.01
87,3.833984375,0.01
88,3.60546875,0.01
89,3.59765625,0.01
90,4.30859375,0.01
91,3.9453125,0.01
92,4.4609375,0.01
93,4.55078125,0.01
94,3.185546875,0.01
95,3.078125,0.01
96,3.42578125,0.01
97,3.81640625,0.01
98,3.263671875,0.01
99,3.0078125,0.01
100,4.25,0.01
